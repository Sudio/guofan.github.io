<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">
    <!-- <link rel="icon" href="../../favicon.ico"> -->

    <title>YNC Machine Learning</title>
 
    <!-- Bootstrap core CSS -->
    <link href="../../bootstrap-3.3.6-dist/css/bootstrap.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="../../jumbotron.css" rel="stylesheet">
  </head>

  <body>

    <nav class="navbar navbar-inverse navbar-fixed-top">
    </nav>

    <!-- Main jumbotron for a primary marketing message or call to action -->
    <div class="jumbotron">
      <div class="container"  style="font-size:16px">
        <h2>YSC3227 Machine Learning</h2>
        <p> Yale-NUS College
	</p>

      </div>
    </div>


    <div class="container" style="font-size:16px">
      
      
	<center>
      <p class="bg-info">
	<br>
      </p>
	</center>
      <br>

      <h3>Description:</h3>
      <p>
      The goal of machine learning is to enable machines/computers
      to  identify 
      patterns from data, extract the patterns, and based on
      them, make
      an inference 
      or prediction automatically. These capabilities are the core of
      artificial intelligence (namely, to make machines learn
      without being explicitly programmed using fixed predetermined rules). 
      The applications of machine learning are immense, since
      nowadays we are bombarded with a huge number of
      various data from various sources. We hope machine learning can make
      sense of this huge seemingly random  data.
      </p>
      <p>
      This course focuses on the fundamentals of machine learning,
      including and deep learning. It should interest students
      who want to study/work in big data, AI (artificial
      intelligence), and data science.
      </p>
      <br>
      <b>Prerequisite</b>: Programming skill in Python.
      <br>
      <b>Textbook</b>: <a href="http://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738/ref=sr_1_1?ie=UTF8&qid=1461689550&sr=8-1&keywords=pattern+recognition+and+machine+learning"
      target="_blank">"Pattern Recognition and Machine Learning"</a>,
      by Christopher Bishop. 

      <br>
      <b>Instructor</b>: <a href="http://tanrobby.github.io">Robby
	T. Tan</a> (robby.tan [att] yale-nus.edu.sg)

      <br>
      <br>
      <hr>
      <h3 id="schedule">Schedule:</h3>
<p>
The following schedule will not be strictly followed.
</p>

<br>
      
<table width="100%" 
       border = "0"
       class="table table-striped">
  <thead class="thead-inverse">
    
    <tr>
      <th width="12%"> Date
      <th> <center> Topic </center>
      <th width="20%"> <center> Lecture Note </center>
  </thead>
  <tbody>
    
    
    <!------------------------------------------------------------------------------>	  
    
    
    <tr>
      <td rowspan="1"> August 14
      <td>
	
	<b>1. INTRODUCTION</b>
	<br>
	<br>
	Additional resources (optional):
	<ul>
	  <li> Introduction to Machine Learning:
	  <a href="http://www.cs.princeton.edu/courses/archive/spr08/cos511/scribe_notes/0204.pdf"
	  target="_blank">pdf</a> | <a href="https://www.youtube.com/watch?v=cKxRvEZd3Mw" target="_blank">youtube</a>
	</ul>
	<br>
      <td align="left">
	<a href="https://www.dropbox.com/s/9db5a66et8urudt/YSC3227_lecture01.pdf?dl=0" target="_blank">Lecture note 1</a>
	<br>
    </tr>
    
     <!------------------------------------------------------------------------------>
    
    
    <tr>
      <td rowspan="1"> August 17
      <td>
	<b> 2. LEAST SQUARES REGRESSION (Part 1/3)</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 1 (Introduction): Sect. 1.1 (Polynomial Curve Fitting)
	</ul>
	<br>
      <td align="left">
	<a href="https://www.dropbox.com/s/ek44fgrc7ap71e2/YSC3227_lecture02.pdf?dl=0" target="_blank">Lecture note 2</a>
	<br>
	<a href="assignment1.html" target="_blank">Assignment 1</a>
    </tr>
	    
    <!------------------------------------------------------------------------------>
    
    <tr>
      <td rowspan="1"> August 21
      <td>
	<b> 3. LEAST SQUARES REGRESSION (Part 2/3)</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Matrix Calculus: <a href="https://en.wikipedia.org/wiki/Matrix_calculus" target="_blank">wikipedia</a>
	</ul>
      <td align="left">
    </tr>
    
    <!------------------------------------------------------------------------------>
    
    <tr>
      <td rowspan="1"> August 24
      <td>
	<b>4.  LEAST SQUARES REGRESSION (Part 3/3)</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li>
	  Overfitting: <a href="https://en.wikipedia.org/wiki/Overfitting"
			  target="_blank">wikipedia</a>
	  <li> Overdetermined
	  systems: <a href="https://en.wikipedia.org/wiki/Overfitting"
		      target="_blank">wikipedia</a>
	  <li> Pseudoinverse: <a href="https://youtu.be/Go2aLo7ZOlU" target="_blank">online tutorial</a>
	</ul>
	<br>
      <td align="left">
    </tr>

    <!------------------------------------------------------------------------------>
    
	
    <tr>
      <td rowspan="1"> August 28
      <td>
	<b>5. INTRODUCTION TO BAYESIAN INFERENCE</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 1 (Introduction): Sect. 1.2 (Probability
	  Theory), Sect. 1.3 (Model Selection)
	  <li> Introduction to Bayesian
	  inference: <a href="http://videolectures.net/mlss09uk_bishop_ibi/?q=bayesian%20inference"
			target="_blank">video</a>
	</ul>
      <td align="left">
	<a href="https://www.dropbox.com/s/uwfxn35xdt5ixy8/YSC3227_lecture05.pdf?dl=0"
	target="_blank">Lecture note 5</a>
    </tr>

    
    <!------------------------------------------------------------------------------>
    
    <tr>
      <td rowspan="1"> August 31
      <td>
	<b>6. MLE AND MAP FOR REGRESSION</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Bayesian Inference: An Introduction to Principles and
	  Practice in Machine
	  Learning (Sect. 2.1, 2.1.1, and 2.1.2 only): <a href="http://www.miketipping.com/papers/met-mlbayes.pdf"
		       target="_blank">pdf</a>
	</ul>
	Additional resources:
	<ul>
	  <li>
	  MLE: <a href="https://www.youtube.com/watch?v=aHwsEXCk4HA"
		  target="_blank">youtube</a>
 	  <li>
 	    MAP: <a href="https://www.youtube.com/watch?v=kkhdIriddSI" target="_blank">youtube</a>
	</ul>
      <td align="left">
	<a href="https://www.dropbox.com/s/vnxs60w8lkvv6ic/YSC3227_lecture06.pdf?dl=0" target="_blank">Lecture note 6</a>
    </tr>
    
    <!------------------------------------------------------------------------------>
    
    
    <tr>
      <td rowspan="1"> September 4
      <td>
	<b>7. BASIS FUNCTIONS</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 3: Linear Models for Regression (Sect. 3.1 only)
	</ul>
	Additional resources:
	<ul>
	  <li> Basis functions: <a href="https://youtu.be/rVviNyIR-fI"
	  target="_blank">youtube 1</a>
	  | <a href="https://youtu.be/wZk_uKEW_Oc" target="_blank">youtube 2</a>
	  <li> Spline
	  functions: <a href="http://geometrie.foretnik.net/files/NURBS-en.swf"
	  target="_blank">demo applet</a> (use Firefox)
	</ul>
	<br>
      <td align="left">
	<a href="assignment2.html" target="_blank">Assignment 2</a>
	<br>
	<a href="https://www.dropbox.com/s/rkyv3qi08th01d3/YSC3227_lecture07.pdf?dl=0" target="_blank">Lecture note 7</a>
    </tr>
    
    <!------------------------------------------------------------------------------>
    
    
    <tr>
      <td rowspan="1"> September 7
      <td>
	<b>8. GAUSSIAN DISTRIBUTIONS: COVARIANCE MATRIX</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 2: Sect. 2.3.1 to 2.3.6 (Gaussian Distribution)
	  <li> Covariance
	    matrix: <a href="https://en.wikipedia.org/wiki/Covariance_matrix"
	    target="_blank">wikipedia</a>
	  <li> Geometric interpretation of covariance
	  matrix: <a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/"
		     target="_blank">website</a>
	</ul>
	<br>
      <td align="left">
	<a href="https://www.dropbox.com/s/i3hfbpfjrhzebn0/YSC3227_lecture08.pdf?dl=0"
	   target="_blank">Lecture note 8</a>
	<br>
	<a href="https://www.dropbox.com/s/3m5hujccx5u778i/YSC3227_lecture08b.pdf?dl=0" target="_blank">Lecture note 8b</a>
    </tr>

    <!------------------------------------------------------------------------------>
	
   <tr>
     <td rowspan="1"> September 11
     <td> 
	<b>9. FULL BAYESIAN REGRESSION AND SEQUENTIAL LEARNING</b>
       <br>
       <br>
	Reading:
	<ul>
	  <li> Chapter 3: Sect. 3.3.1 (Parameter Distribution)
	</ul>
	<br>
     <td align="left">
       <a href="https://www.dropbox.com/s/stpvv6opv9mkaq5/YSC3227_lecture09.pdf?dl=0" target="_blank">Lecture note 9</a>
    </tr>
    <!------------------------------------------------------------------------------>
    
    
    <tr>
      <td rowspan="1"> September 14
      <td>
       <b>10. PREDICTIVE DISTRIBUTION</b> 
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 3: Sect. 3.3.2 (Predictive Distribution)
	  <li> Bayesian Inference: An Introduction to Principles and
		    Practice in Machine
		    Learning (From Section 1 to Section 3 only): 
		    <a href="http://www.miketipping.com/papers/met-mlbayes.pdf"
		       target="_blank">pdf</a>
	</ul>
	<br>
	
      <td align="left">
    <!------------------------------------------------------------------------------>

    
    <tr>
      <td rowspan="1">  September 18
      <td>
	<b> 11. GAUSSIAN PROCESSES </b>
	<br>
	<br>
      <td align="left">
    </tr>

    <!------------------------------------------------------------------------------>

    
    <tr>
      <td rowspan="1">  September 21
      <td>
	<b>12.  REVIEW: BAYESIAN METHODS FOR REGRESSION</b> 
	<br>
	<br>
      <td align="left">
    </tr>
    
    <!------------------------------------------------------------------------------>

    <tr class="success">
      
      <td rowspan="1"> 
      <td>
	<b> RECESS WEEK </b>
	<br>
	<br>		
      <td>
    </tr>
    
    <!------------------------------------------------------------------------------>

    <tr>
      
      <td rowspan="1"> October 2
      <td>
	<b>13. LEAST-SQUARES CLASSIFICATION (Part 1/2)</b>
	<br>
	<br>
      <td align="left">
    </tr>


    <!------------------------------------------------------------------------------>


	
    <tr>
      <td rowspan="1"> October 5
      <td>
	<b>14. LEAST-SQUARES FOR CLASSIFICATION (Part 2/2)</b>
	<br>
	<br>	
      <td align="left">
    </tr>
    
    <!------------------------------------------------------------------------------>

	
    <tr>
      <td rowspan="1"> October 9
      <td>
	<b>15. PROBABILISTIC CLASSIFICATION: MLE</b>
	<br>
	<br>
      <td align="left">
    </tr>

    <!------------------------------------------------------------------------------>

    
    <tr>
      <td rowspan="1"> October 12
      <td>
 	<b>16. LAPLACE APPROXIMATION</b>
	<br>
	<br>
      <td align="left">
    </tr>
    
    <!------------------------------------------------------------------------------>

    
	
    <tr>
      <td rowspan="1"> October 16
      <td>
	<b>17. BAYESIAN LOGISTIC REGRESSION</b>
	<br>
	<br>
      <td align="left">
    </tr>

    <!------------------------------------------------------------------------------>
    
    
    
    <tr>
      <td rowspan="1"> October 19
      <td>
	<b>18. GAUSSIAN PROCESSES FOR CLASSIFICATION (Part 1/2)</b>
	<br>
	<br>
      <td align="left">
    </tr>

    <!------------------------------------------------------------------------------>

    
    
    <tr>
      <td rowspan="1"> October 23
      <td>
	<b>19. GAUSSIAN PROCESSES FOR CLASSIFICATION (Part 2/2)</b>
	<br>
	<br>
      <td align="left">
    </tr>
    
    <!------------------------------------------------------------------------------>
    
    
    
    <tr>
      <td rowspan="1"> October 26
      <td>
	<b>20. REVIEW: BAYESIAN METHODS FOR CLASSIFICATION</b>
	<br>
	<br>
      <td align="left">
    </tr>

    <!------------------------------------------------------------------------------>

	
    <tr>
	<td rowspan="1"> October 30
	<td>
	  <b>21. SVM (SUPPORT VECTOR MACHINES) Part 1/2</b>
	  <br>
	  <br>
	<td align="left">
    </tr>

    <!------------------------------------------------------------------------------>
	  
    
    <tr>
      <td rowspan="1"> November 2
      <td>
	  <b>22. SVM (SUPPORT VECTOR MACHINES) Part 1/2</b>
	  <br>
	  <br>
      <td align="left">
    </tr>
    
    <!------------------------------------------------------------------------------>
    
    
    <tr>
      <td rowspan="1"> November 9
      <td>
	<b>23. MIXTURE MODELS AND EM (Part 2/2)</b>
	<br>
	<br>
      <td align="left">
    </tr>
    
    <!------------------------------------------------------------------------------>

    <tr>
      <td rowspan="1"> November 13
      <td>
	<b>24. MIXTURE MODELS AND EM (Part 1/2)</b>
	<br>
	<br>
      <td align="left">
    </tr>
    
    <!------------------------------------------------------------------------------>

    <tr>
      <td rowspan="1"> November 16
      <td>
	<b>25. PCA </b> 
	<br>
	<br>
      <td align="left">
    </tr>

    <!------------------------------------------------------------------------------>

    <tr>
      <td rowspan="1"> November 20
      <td>
	<b>26. RANDOM FORESTS</b>
	<br>
	<br>
      <td align="left">
    </tr>


    <!------------------------------------------------------------------------------>

    <tr>
      <td rowspan="1"> November 23
      <td>
	<b>27. REVIEW AND DISCUSSION</b>
	<br>
	<br>
      <td align="left">
    </tr>

    
    <!------------------------------------------------------------------------------>
    

    <tr class="danger">
      <td rowspan="1"> 
      <td>
	<b>FINAL EXAM</b>
	<br>
	<br>

	<td align="left"> 
    </tr>


    
  </tbody>
</table>
<br>
      


<!------------------------------------------------------------------------------>
<!------------------------------------------------------------------------------>
<!------------------------------------------------------------------------------>
<!------------------------------------------------------------------------------>

<br>
<br>

      <hr>
      <h3 id="syllabus">Syllabus:</h3>
      <br>
      <ul>
	<li> Least Squares for Regression
	<li> Bayes' Theorem
	<li> MLE and MAP for Regression
	<li> BasisFunctions
	<li> Gaussian Distributions
	<li> Bayesian Sequential Learning
	<li> Predictive Distribution
	<li> Gaussian Processes for Regression
	<li> Least Squares for Classification
	<li> Bayesian Logistic Regression
	<li> Gaussian Processes for Classification
	<li> Support Vector Machine
	<li> Mixture Models and EM Algorithm
	<li> Principle Component Analysis
	<li> Random Forests
      </ul>

<br>
<hr>
<br>
<br>
<br>

      

</div>      
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="bootstrap-3.3.6-dist/js/bootstrap.min.js"></script>
  </body>
  <script type="text/javascript">
    var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
  </script>
<script type="text/javascript">
  try {
  var pageTracker = _gat._getTracker("UA-13131132-3");
  pageTracker._trackPageview();
  } catch(err) {}</script>
</html>
