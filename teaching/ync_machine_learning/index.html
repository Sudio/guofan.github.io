<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">
    <!-- <link rel="icon" href="../../favicon.ico"> -->

    <title>YNC Machine Learning</title>

    <!-- Bootstrap core CSS -->
    <link href="../../bootstrap-3.3.6-dist/css/bootstrap.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="../../jumbotron.css" rel="stylesheet">
  </head>

  <body>

    <nav class="navbar navbar-inverse navbar-fixed-top">
    </nav>

    <!-- Main jumbotron for a primary marketing message or call to action -->
    <div class="jumbotron">
      <div class="container"  style="font-size:16px">
        <h2>YSC3227 Machine Learning</h2>
        <p>Semester 2, 2016/2017
	  <br>
          Yale-NUS College
	</p>

      </div>
    </div>


    <div class="container" style="font-size:16px">
      
      
	<center>
      <p class="bg-info">
	<br>
      </p>
	</center>
      <br>

      <h3>Description:</h3>
      <p>
      The goal of machine learning is to enable machines/computers
      to  identify 
      patterns from data, extract the patterns, and based on
      them, make
      an inference 
      or prediction automatically. These capabilities are the core of
      artificial intelligence (namely, to make machines learn
      without being explicitly programmed using specific fixed rules). 
      The applications of machine learning are immense, since
      nowadays we are bombarded with a huge number of
      various data from various sources. We hope machine learning can make
      sense of this huge seemingly random  data.
      </p>
      <p>
      Technically, there are at least three types of learning:
      supervised, unsupervised, and reinforcement learning. Supervised
      learning means we use both data and their
      annotations or ground truths to train an algorithm. Usually, the annotations are provided by us
      (humans). Unsupervised learning means we train an algorithm using
      only raw data, without any manual annotations. In this case, the
      algorithm tries to find and extract the most significant
      patterns from 
      the data by itself. Reinforcement learning means an algorithm is
      trained using data and feedback/rewards from its actions. Thus,
      there are some interactions between the algorithm and its
      environments. 
      </p>
      <p>
      This course focuses on these three types of learning, including
      shallow learning and deep learning. It should interest students
      who want to study/work in big data, AI (artificial
      intelligence), and data science.
      </p>
      <br>
      <b>Prerequisite</b>: Programming skill in Python.
      <br>
      <b>Textbook</b>: <a href="http://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738/ref=sr_1_1?ie=UTF8&qid=1461689550&sr=8-1&keywords=pattern+recognition+and+machine+learning"
      target="_blank">"Pattern Recognition and Machine Learning"</a>,
      by Christopher Bishop. 

      <br>
      <b>Instructor</b>: <a href="http://tanrobby.github.io">Robby
      T. Tan</a> (robby.tan [att] yale-nus.edu.sg)
      <br>
      <br>
      <hr>
      <h3 id="schedule">Schedule:</h3>
<p>
The following schedule will not be strictly followed.
</p>

<br>
      
<table width="100%" 
       border = "0"
       class="table table-striped">
  <thead class="thead-inverse">
    
    <tr>
      <th width="12%"> Date
      <th> <center> Topic </center>
      <th width="20%"> <center> Lecture Note </center>
  </thead>
  <tbody>
    
    
    <!------------------------------------------------------------------------------>	  
    
    
    <tr>
      <td rowspan="1"> January 10
      <td>
	
	<b>1. LEAST SQUARES</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 1: Introduction (Sect. 1.1)
	</ul>
	<br>
	Additional reading (optional):
	<ul>
	  <li> Introduction to Machine Learning:
	  <a href="http://www.cs.princeton.edu/courses/archive/spr08/cos511/scribe_notes/0204.pdf"
	  target="_blank">pdf</a> 
	</ul>
	<br>
      <td align="left">
	<a href="https://www.dropbox.com/s/4wb6w8cudbq2pli/YSC3227_lecture01.pdf?dl=0" target="_blank">Lecture Note 1</a>
	<br>
	<a href="assignment1.html" target="_blank">Assignment 1</a>

    </tr>
    
    <!------------------------------------------------------------------------------>
    
    
    <tr>
      <td rowspan="1"> January 13
      <td>
	<b> 2. INTRO TO BAYESIAN INFERENCE</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 1: Introduction (Sect. 1.2)
	  <li> Introduction to Bayesian
	  inference: <a href="http://videolectures.net/mlss09uk_bishop_ibi/?q=bayesian%20inference"
			target="_blank">video</a>
	</ul>
	<br>
      <td>
	<a href="https://www.dropbox.com/s/56bknozpl43hocw/YSC3227_lecture02.pdf?dl=0" target="_blank">Lecture Note 2</a>
    </tr>
	    
    <!------------------------------------------------------------------------------>
    
    <tr>
      <td rowspan="1"> January 17
      <td>
	<b>3. MLE AND MAP FOR REGRESSION</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Bayesian Inference: An Introduction to Principles and
	  Practice in Machine
	  Learning (Sect. 2.1, 2.1.1, and 2.1.2 only): <a href="http://www.miketipping.com/papers/met-mlbayes.pdf"
		       target="_blank">pdf</a>
	</ul>
	<br>
	
      <td align="left">
	<a href="https://www.dropbox.com/s/unyv6twhyh5pric/YSC3227_lecture03.pdf?dl=0" target="_blank"> Lecture Note 3</a>
	<br>
	<a href="assignment2.html" target="_blank">Assignment 2</a>
    </tr>
    
    <!------------------------------------------------------------------------------>
    
    <tr>
      <td rowspan="1"> January 20
      <td>
	<b>4. BASIS FUNCTIONS</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 3: Linear Models for Regression (Sect. 3.1 only)
	  <li> Spline functions: <a href="http://geometrie.foretnik.net/files/NURBS-en.swf" target="_blank">Demo</a>
	</ul>
	<br>		
      <td align="left">
	<a href="https://www.dropbox.com/s/2uy8nv7c1vmy7la/YSC3227_lecture04.pdf?dl=0" target="_blank">Lecture Note 4</a>
    </tr>

    <!------------------------------------------------------------------------------>
    
	
    <tr>
      <td rowspan="1"> January 24
      <td>
	<b>5. GAUSSIAN DISTRIBUTIONS: COVARIANCE MATRIX</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Covariance
	    matrix: <a href="https://en.wikipedia.org/wiki/Covariance_matrix"
	    target="_blank">wikipedia</a>
	  <li> Geometric interpretation of covariance
	  matrix: <a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/"
	  target="_blank">website</a>
	    
	</ul>
	<br>
      <td align="left">
	<a href="https://www.dropbox.com/s/uhdbitndlfw4e7e/YSC3227_lecture05.pdf?dl=0" target="_blank">Lecture Note 5</a>
    </tr>

    
    <!------------------------------------------------------------------------------>
    
    <tr>
      <td rowspan="1"> January 27
      <td>
	<b>6. CONDITIONAL AND MARGINAL GAUSSIAN DISTRIBUTIONS</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 2: Sect. 2.3 (The Gaussian Distributions):
	    from Sect. 2.3.1 to Sect. 2.3.2. <em>(Note: read lecture
	    note 5 before reading this section)</em>
	</ul>
	<br>
      <td align="left">
	<a href="https://www.dropbox.com/s/mv184sssnxx0xef/YSC3227_lecture06.pdf?dl=0"
	   target="_blank">Lecture Note 6</a>
    </tr>
    
    <!------------------------------------------------------------------------------>
    
    
    <tr>
      <td rowspan="1"> January 31
      <td>
	<b>7. BAYES' THEOREM FOR GAUSSIAN VARIABLES</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 2: Sect. 2.3.3 (Bayes' theorem for Gaussian variables)
	</ul>
	<br>
      <td align="left">
	<a href="https://www.dropbox.com/s/icufnamk37pmats/YSC3227_lecture07.pdf?dl=0" target="_blank">Lecture Note 7</a>
    </tr>
    
    <!------------------------------------------------------------------------------>
    
    
    <tr>
      <td rowspan="1"> February 3
      <td>
	<b>8. SEQUENTIAL BAYESIAN LEARNING</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 3: Sect. 3.3.1 (Parameter Distribution)
	</ul>
	<br>
	
      <td align="left">
	<a href="https://www.dropbox.com/s/8fitwnduol5gnii/YSC3227_lecture08.pdf?dl=0"
	   target="_blank">Lecture Note 8</a>
	<br>
	<a href="assignment3.html" target="_blank">Assignment 3</a>
    </tr>

    <!------------------------------------------------------------------------------>
	
   <tr>
     <td rowspan="1"> February 7
     <td> 
       <b>9. REVIEW: METHODS FOR ESTIMATING W</b>
       <br>
       <br>
     <td align="left">
       <a href="https://www.dropbox.com/s/amj4orkk45xihoz/YSC3227_lecture09.pdf?dl=0" target="_blank">Lecture Note 9</a>
    </tr>
    <!------------------------------------------------------------------------------>
    
    
    <tr>
      <td rowspan="1"> February 10
      <td>
	<b>10. PREDICTIVE DISTRIBUTION</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 3: Sect. 3.3.2 (Predictive Distribution)
	</ul>
	<br>
      <td align="left">
	<a href="https://www.dropbox.com/s/7wh1ufktwygkcrd/YSC3227_lecture10.pdf?dl=0"
	   target="_blank">Lecture Note 10</a>
	<br>
	<a href="assignment4.html" target="_blank">Assignment 4</a>
    </tr>
    <!------------------------------------------------------------------------------>

    
    <tr>
      <td rowspan="1"> February 14
      <td>
	<b>11. GAUSSIAN PROCESSES 1</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 6: Sect. 6.4.1 (Linear Regression Revisited)
	  and Sect. 6.4.2 (Gaussian Processes for Regression)
	</ul>
	<br>
	Additional resources:
	<ul>
	  <li> Gaussian
	  Processes: <a href="https://www.youtube.com/watch?v=vU6AiEYED9E"
	  target="_blank">youtube</a> 
	</ul>
      <td align="left">
	<a href="https://www.dropbox.com/s/dkg1wouuy92gzmh/YSC3227_lecture11.pdf?dl=0" target="_blank">Lecture Note 11</a>
    </tr>

    <!------------------------------------------------------------------------------>

    <tr>
      
      <td rowspan="1"> February 17
      <td>
	<b>12. GAUSSIAN PROCESSES 2</b>
	<br>
	<br>
      <td align="left">
	see Lecture Note 11
    </tr>

    
    <!------------------------------------------------------------------------------>

    <tr class="success">
      
      <td rowspan="1"> 
      <td>
	<b> RECESS WEEK </b>
	<br>
	<br>		
      <td>
    </tr>

    <!------------------------------------------------------------------------------>


	
    <tr>
      <td rowspan="1"> February 28
      <td>
 	<b>13. CLASSIFICATION: LEAST SQUARES</b>
	<br>
	Reading:
	<ul>
	  <li> Chapter 4: Sect. 4.1.1 (two classes), 4.1.2 (multiple classes),
	  4.1.3 (least squares for classification)
	</ul>
	<br>
      <td align="left">
	<a href="assignment5.html" target="_blank">Assignment 5</a>
	<br>
	<a href="https://www.dropbox.com/s/h3o3xp7658jdzkt/YSC3227_lecture13.pdf?dl=0" target="_blank">Lecture Note 13</a>
    </tr>

    <!------------------------------------------------------------------------------>

	
    <tr>
      <td rowspan="1"> March 3
      <td>
	<b>14. FISHER'S LINEAR DISCRIMINANT ANALYSIS</b>
	<br>
	Reading:
	<ul>
	  <li> Chapter 4: Sect. 4.1.4 (Fisher's linear discriminant).

	</ul>
      <td align="left">
	<a href="https://www.dropbox.com/s/yuf62cv09otonow/YSC3227_lecture14.pdf?dl=0" target="_blank">Lecture Note 14</a>
    </tr>

    <!------------------------------------------------------------------------------>

    
    <tr>
      <td rowspan="1"> March 7
      <td>
	Quiz 1 
	<br>
	<br>
      <td align="left">
	<a href="assignment6.html" target="_blank">Assignment 6</a>
	<br>
    </tr>
    <!------------------------------------------------------------------------------>
    
    
    <tr>
      <td rowspan="1"> March 14
      <td>
	<b>15. DETAILS OF FISHER'S LDA</b>
	<br>
	<br>
	
      <td align="left">
	<a href="https://www.dropbox.com/s/tvqodwxznyrma8p/YSC3227_lecture15.pdf?dl=0" target="_blank">Lecture Note 15</a>
	<br>
	Quiz 2
    </tr>
    
    <!------------------------------------------------------------------------------>

    


	
    <tr>
      <td rowspan="1"> March 17
      <td>
	<b>16. PROBABILISTIC GENERATIVE + DISCRIMINATIVE MODELS</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 4: Sect. 4.2 (Probabilistic Generative Models),
	  particularly Sect. 4.2.2 (maximum likelihood solution), Sect. 4.3 (Probabilistic Discriminative
	    Models), particularly Sect. 4.3.2 (logistic regression)
	    and 4.3.3 (iterative reweighted least squares)
	</ul>
      <td align="left">
	<a href="https://www.dropbox.com/s/ge9nqj1lxyim3g7/YSC3227_lecture16.pdf?dl=0" target="_blank">Lecture Note 16</a>
    </tr>

    <!------------------------------------------------------------------------------>
    
    
    
    <tr>
      <td rowspan="1"> March 21
      <td>
	<b>17. BAYESIAN LOGISTIC REGRESSION</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 4: Sect. 4.5 (Bayesian logistic regression)
	</ul>
      <td align="left">
	<a href="https://www.dropbox.com/s/olhjx7lgerag3kv/YSC3227_lecture17.pdf?dl=0" target="_blank">Lecture Note 17</a>
    </tr>

    <!------------------------------------------------------------------------------>

    
    
    <tr>
      <td rowspan="1"> March 24
      <td>
	<b>18. GAUSSIAN PROCESSES FOR CLASSIFICATION</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 6: Sect. 6.4.5 (Gaussian processes for
	  classification), Sect. 6.4.6 (Laplace approximation)
	</ul>
      <td align="left">
	<a href="assignment7.html" target="_blank">Assignment 7</a>
    </tr>
    
    <!------------------------------------------------------------------------------>
    
    
    
    <tr>
      <td rowspan="1"> March 28
      <td>
      <td align="left">
    </tr>

    <!------------------------------------------------------------------------------>

	
    <tr>
	<td rowspan="1"> March 31
	<td>
	<td align="left">


    </tr>

    <!------------------------------------------------------------------------------>
	  
    
    <tr>
      <td rowspan="1"> April 4
      <td>
      <td align="left">
    </tr>
    
    <!------------------------------------------------------------------------------>
    
    
    <tr>
      <td rowspan="1"> April 7
      <td>
      <td align="left"> 
    </tr>
    
    <!------------------------------------------------------------------------------>

	
    <tr>
      <td rowspan="1"> April 11
      <td>
	<b>25. REVIEW + QA SESSION</b>
	<br>
	<br>
      <td align="left"> 
    </tr>
    
    <!------------------------------------------------------------------------------>
	

    <tr class="danger">
      <td rowspan="1"> May 2
      <td>
	<b>FINAL EXAM <b>
	<br>
	<br>
	
	<td align="left"> 
    </tr>


    
  </tbody>
</table>
<br>


<!------------------------------------------------------------------------------>
<!------------------------------------------------------------------------------>
<!------------------------------------------------------------------------------>
<!------------------------------------------------------------------------------>

<br>
<br>

      <hr>
      <h3 id="syllabus">Syllabus:</h3>
      <br>
      <ul>
	<li> Bayesian Inference
	<li> Linear Models for Regression
	<li> Kernel Methods (Gaussian Processes)
	<li> Linear Models for Classification
	<li> Sparse Kernel Machines
	<li> Approximate Inference
	<li> Ensemble Learning
	<li> Neural Networks and Deep Learning
	<li> Mixture Models and EM
	<li> Sampling Methods
	<li> Continuous Latent Variables
	<li> Sequential Data
	<li> Reinforcement Learning
      </ul>

<br>
<hr>
<br>
<br>
<br>

      

</div>      
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="bootstrap-3.3.6-dist/js/bootstrap.min.js"></script>
  </body>
  <script type="text/javascript">
    var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
  </script>
<script type="text/javascript">
  try {
  var pageTracker = _gat._getTracker("UA-13131132-3");
  pageTracker._trackPageview();
  } catch(err) {}</script>
</html>
